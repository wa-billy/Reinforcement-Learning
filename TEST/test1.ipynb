{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e85452",
   "metadata": {},
   "source": [
    "### 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3746e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "maze = np.array([\n",
    "    [0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    [0, 0, 0, 0, 1, 0, 0, 0, 0, 1],\n",
    "    [1, 1, 1, 0, 1, 0, 1, 1, 0, 1],\n",
    "    [1, 0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
    "    [1, 0, 1, 1, 1, 1, 1, 0, 1, 1],\n",
    "    [1, 0, 1, 0, 0, 0, 0, 0, 1, 1],\n",
    "    [1, 0, 1, 0, 1, 1, 1, 0, 1, 1],\n",
    "    [1, 0, 1, 0, 1, 0, 0, 0, 1, 1],\n",
    "    [1, 0, 1, 0, 1, 0, 1, 0, 0, 1],\n",
    "    [1, 1, 1, 0, 1, 1, 1, 1, 0, 0]\n",
    "])\n",
    "\n",
    "start = (0, 0)\n",
    "goal = (9, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960ecb79",
   "metadata": {},
   "source": [
    "### 2. Set RL parameter and Q table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9568c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# จำนวนครั้งที่ agent พยายามหาค่าผ่านเขาวงกต\n",
    "num_episodes = 5000\n",
    "\n",
    "# การเรียนรู้ที่จะควบคุมว่าข้อมูลใหม่จะเข้ามาแทนที่ข้อมูลเก่ามากน้อยเพียงใด\n",
    "alpha = 0.1\n",
    "\n",
    "# ปัจจัยส่วนลดที่ให้ความสำคัญกับผลตอบแทนในทันนีมากกว่า\n",
    "gamma = 0.9\n",
    "\n",
    "# ความน่าจะเป็นของการสำรวจเทียบกับการใช้ประโยชน์ โดยค่าเอปซิลอนจะสูงกว่าหากสำรวจมากกว่า\n",
    "epsilon = 0.5\n",
    "\n",
    "\n",
    "reward_fire = -10\n",
    "reward_goal = 50\n",
    "reward_step = -1\n",
    "\n",
    "actions = [(0, -1), (0, 1), (-1, 0), (1, 0)]\n",
    "\n",
    "# Q  คือตาราง Q ที่เริ่มต้นด้วยค่าศูนย์ โดยจะเก็บค่ารางวัลที่คาดหวังสำหรับแต่ละคู่สถานะ-การกระทำ\n",
    "Q = np.zeros(maze.shape + (len(actions), ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ed5da6",
   "metadata": {},
   "source": [
    "### 3. Detected functions and choose actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b709adeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_valid ช่วยให้มั่นใจได้ว่าเอเจนต์สามารถเคลื่อนที่ได้เฉพาะภายในเขาวงกตและหลีกเลี่ยงสิ่งกีดขวางเท่านั้น\n",
    "def is_valid(pos):\n",
    "    r, c = pos\n",
    "    if r < 0 or r >= maze.shape[0]:\n",
    "        return False\n",
    "    if c < 0 or c >= maze.shape[1]:\n",
    "        return False\n",
    "    if maze[r, c] == 1:\n",
    "        return False\n",
    "    \n",
    "# choose_action เป็นการนำกลยุทธ์การสำรวจ (การกระทำแบบสุ่ม) กับการใช้ประโยชน์ (การกระทำที่เรียนรู้ได้ดีที่สุด) มาใช้\n",
    "def choose_action(state):\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.randint(len(actions))\n",
    "    else:\n",
    "        return np.argmax(Q[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b5787b",
   "metadata": {},
   "source": [
    "### 4. Training agent with Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648e4aab",
   "metadata": {},
   "source": [
    "Q-Learning rule:\n",
    "    ![alt text](<สกรีนช็อต 2026-01-06 021935.png>)\n",
    "    ![alt text](<สกรีนช็อต 2026-01-06 022102-1.png>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5535b562",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_all_episodes = []\n",
    "\n",
    "for epidose in range(num_episodes):\n",
    "    state = start\n",
    "    total_rewards = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action_index = choose_action(state)\n",
    "        action = actions[action_index]\n",
    "\n",
    "        next_state = (state[0] + action[0], state[1] + action[1])\n",
    "\n",
    "        if not is_valid(next_state):\n",
    "            reward = reward_fire\n",
    "            done = True\n",
    "        elif next_state == goal:\n",
    "            reward = reward_goal\n",
    "            done = True\n",
    "        else:\n",
    "            reward = reward_step\n",
    "\n",
    "        old_value = Q[state][action_index]\n",
    "        next_max = np.max(Q[next_state]) if is_valid(next_state) else 0\n",
    "\n",
    "        Q[state][action_index] = old_value + alpha * \\\n",
    "            (reward + gamma * next_max - old_value)\n",
    "\n",
    "        state = next_state\n",
    "        total_rewards += reward\n",
    "\n",
    "    global epsilon\n",
    "    epsilon = max(0.01, epsilon * 0.995)\n",
    "    reward_all_episodes.append(total_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f66f23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
